Hello everyone and welcome to our presentation about our project on Statistical Foundations of Machine Learning. Our group consists of three students from Computer Sciences : Antoine, Pierre and I, Nathan. I will start by presenting the first part of the project : Data Preprocessing. Then, Pierre will show you which model we used and finally Antoine will explain the third part about an alternative model : [TODO].

Our project is about predicting if a water pump is functional, requires maintenance or is not functionnal given a plentitude of data about the pump such as its position, its name, the amount of water available, the funder, the payment type, and so on. 
Any result with an error rate below 66% will be considered a success, since it means our data gives us enough information to make an informed decision, and not just a random one.
Now, obviously we do not need to know for example the name of the waterpoint to decide if it is functionnal or not, so we will have to preprocess the data, that is modify or remove the columns or row that we find detrimental to the learning.

First of all, we looked with our own eyes in the data and noticed that one column always had the same value. This doesn't bring us any information about the pump since we can guess it so we can simply remove the column.

This can be generalized to not only 1 value but also columns which one value appear very often. With this cell we can get a dataframe ordered by the ratio of the most common value to the others. We see that the column num_private has one value appearing almost 99 percent of the time. With that information, we will assume that it appears 100 percent of the time without losing a lot of information, but this gives us an advantage by reducing the size of the input space by a bit

Now we are going to look at columns pairwise because we noticed sometimes the information was incoded in two different column for exemple quantity and quantity_group. By printing the contingency table with the table command, we see that the two columns are indeed identical. This means we can remove one of the two. For some other pairs of columns, we have that one column brings less information (less possible outcomes) than the other, so we will keep the other column

However this process is not very scalable since it requires looking at the table of all pairs of columns and we have a lot of that. To solve that, we can use chisq.test that will give us three parameters for the correlation between the two columns, but since we don't know a lot about chi square, we won't really touch the data only because the test said so. What we ended up doing is that when we suspected a high correlation with the chisq test, we looked at the table and decided if the removal of a column was necessary

[todo explain chisq test and contingency table]

[todo explain missing value imputation]

--- Pierre ---
--- Antoine ---